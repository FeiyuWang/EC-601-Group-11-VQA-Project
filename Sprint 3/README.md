# Dataset
## VizWiz
VizWiz is the first target-oriented VQA data set generated by natural VQA settings. VizWiz contains more than 31,000 visual questions from blind people. Every blind person took a photo with his mobile phone and recorded a verbal question about it. Each visual question also had 10 crowdsourced answers. VizWiz is different from many existing VQA datasets in that the images are taken by blind photographers, so the quality is usually poor, the questions are expressed verbally, so they are more conversational and often unable to answer visual questions. The evaluation of modern algorithms used to answer visual questions and determine whether visual questions are answerable shows that VizWiz is a challenging data set. The data set is designed to encourage a larger community to develop more general algorithms that can help blind people.[1]

20,523 training image/question pairs
205,230 training answer/answer confidence pairs
4,319 validation image/question pairs
43,190 validation answer/answer confidence pairs
8,000 test image/question pairs

![image](https://user-images.githubusercontent.com/90427304/141823070-372f7806-c1de-493c-99e9-e81e464017a3.png)


# Reference
[1] https://arxiv.org/abs/1802.08218v2
