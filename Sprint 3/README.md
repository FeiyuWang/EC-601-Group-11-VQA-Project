# Dataset
## VizWiz
VizWiz is the first target-oriented VQA data set generated by natural VQA settings. VizWiz contains more than 31,000 visual questions from blind people. Every blind person took a photo with his mobile phone and recorded a verbal question about it. Each visual question also had 10 crowdsourced answers. VizWiz is different from many existing VQA datasets in that the images are taken by blind photographers, so the quality is usually poor, the questions are expressed verbally, so they are more conversational and often unable to answer visual questions. The evaluation of modern algorithms used to answer visual questions and determine whether visual questions are answerable shows that VizWiz is a challenging data set. The data set is designed to encourage a larger community to develop more general algorithms that can help blind people.[1]

* 20,523 training image/question pairs
* 205,230 training answer/answer confidence pairs
* 4,319 validation image/question pairs
* 43,190 validation answer/answer confidence pairs
* 8,000 test image/question pairs

![image](https://user-images.githubusercontent.com/90427304/141823070-372f7806-c1de-493c-99e9-e81e464017a3.png)[2]

# Text-to-Speech
We update the demo of text-to-speech function, which is from gTTS. The gTTS is from Google Text-to-Speech, which is a screen reader application developed by Google for the Android operating system. It powers applications to read aloud (speak) the text on the screen with support for many languages. This helps blind people to get voice guide when the VQA model output answers in text form.

Here is the install code for gTTS
```
pip install gTTS
from gtts import gTTS
import os
language = 'en'
mytext = "testing"
myobj = gTTS(text=mytext, lang=language, slow=False)
myobj.save("welcome.mp3")
os.system("mpg321 welcome.mp3")
```
# Reference
[1] https://arxiv.org/abs/1802.08218v2

[2] https://vizwiz.org/tasks-and-datasets/vqa/
