# Dataset

The dataset used in VQA model is VQA2.0.

Visual Question Answering (VQA) v2.0 is a dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer. It is the second version of the VQA dataset[1].

* 265,016 images (COCO and abstract scenes)
* At least 3 questions (5.4 questions on average) per image
* 10 ground truth answers per question
* 3 plausible (but likely incorrect) answers per question
* Automatic evaluation metric

The most commonly used dataset is VQA2.0

![image](https://user-images.githubusercontent.com/90427304/139571469-3234837c-41ac-4385-8b58-dcaa39bcf122.png)

Single model VQA 2.0 and VizWiz performance in %

![image](https://user-images.githubusercontent.com/90427304/139571528-70a80f46-0fcf-40ae-9843-575e8ff56878.png)

# Reference
[1] https://paperswithcode.com/dataset/visual-question-answering-v2-0

[2] https://arxiv.org/pdf/1904.08920v2.pdf
