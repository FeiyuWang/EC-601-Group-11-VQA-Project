{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXeEePmeEuuY"
   },
   "source": [
    "## First download all of the necessary data\n",
    "\n",
    "---\n",
    "\n",
    "Press \"Shift + Enter\" to run each cell sequentially. Alternatively, you can press \"Cmd/Ctrl + F9\" to run all cells and then scroll down to bottom cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRUp0r_-B9N0",
    "outputId": "6e39251b-bd66-48f7-b4f0-b717c673a997"
   },
   "outputs": [],
   "source": [
    "# Download Pre-requisites needed for running the e2e model\n",
    "%cd content\n",
    "%mkdir model_data\n",
    "import urllib.request\n",
    "url = 'https://dl.fbaipublicfiles.com/pythia/data/answers_vqa.txt'\n",
    "filename = '../model_data/answers_vqa.txt'\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "url = 'https://dl.fbaipublicfiles.com/pythia/data/vocabulary_100k.txt'\n",
    "filename = '../model_data/vocabulary_100k.txt'\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "url = 'https://dl.fbaipublicfiles.com/pythia/data/vocabulary_100k.txt'\n",
    "filename = '../model_data/vocabulary_100k.txt'\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "url = 'https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model.pth'\n",
    "filename = '../model_data/detectron_model.pth'\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "url = 'https://dl.fbaipublicfiles.com/pythia/pretrained_models/vqa2/pythia_train_val.pth'\n",
    "filename = '../model_data/pythia.pth'\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "url = 'https://dl.fbaipublicfiles.com/pythia/pretrained_models/vqa2/pythia_train_val.yml'\n",
    "filename = '../model_data/pythia.yaml'\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "url = 'https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model.yaml'\n",
    "filename = '../model_data/detectron_model.yaml'\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "url = 'https://dl.fbaipublicfiles.com/pythia/data/detectron_weights.tar.gz'\n",
    "filename = '../model_data/detectron_weights.tar.gz'\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4S01cUqE3WJ"
   },
   "source": [
    "## Now, install some particular dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xf ~/content/model_data/detectron_weights.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rQCyXjYyFQzp",
    "outputId": "7c7a5203-6e3a-411d-9923-4ba8d15ad24e"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install yacs cython matplotlib\n",
    "!pip install git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
    "!pip3 install --upgrade Pillow\n",
    "!pip install gTTS\n",
    "!pip install pygame\n",
    "!pip install -q torchaudio omegaconf soundfile\n",
    "!pip install ffmpeg\n",
    "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n",
    "!pip install pyaudio\n",
    "!pip install .\\PyAudio-0.2.11-cp38-cp38-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install .\\PyAudio-0.2.11-cp39-cp39-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivSdn9BFFpxp"
   },
   "source": [
    "## Install MMF now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FOHchoDW7yqa",
    "outputId": "597e995b-d413-40a9-a1fc-06ab9da1ecf4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd ~/content/\n",
    "!git clone https://github.com/facebookresearch/mmf.git mmf\n",
    "%cd ~/content/mmf\n",
    "!pip install -e .\n",
    "import sys\n",
    "sys.path.append(\"~/content/mmf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd ~/content/\n",
    "!git clone https://github.com/facebookresearch/mmf.git mmf\n",
    "%cd ~/content/mmf\n",
    "!pip install --editable . --user\n",
    "import sys\n",
    "sys.path.append(\"~/content/mmf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5o-zqvuFxeR"
   },
   "source": [
    "## Install maskrcnn-benchmark now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install libpython m2w64-toolchain -c msys2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall apex\n",
    "%cd apex\n",
    "!python setup.py install --cuda_ext --cpp_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V1ROyH7yG11V",
    "outputId": "e7f1133d-6b8e-4e2c-b875-7e9ca7b56802",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\53444\\content\n",
      "C:\\Users\\53444\\content\\vqa-maskrcnn-benchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vqa-maskrcnn-benchmark' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running build\n",
      "running build_py\n",
      "running build_ext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\53444\\anaconda3\\lib\\site-packages\\torch\\utils\\cpp_extension.py:316: UserWarning: Error checking compiler version for g++: Command 'g++' returned non-zero exit status 1.\n",
      "  warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "error: [WinError 2] 系统找不到指定的文件。\n",
      "Error in atexit._run_exitfuncs:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\53444\\anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py\", line 59, in closed\n",
      "    return stream.closed\n",
      "ValueError: underlying buffer has been detached\n"
     ]
    }
   ],
   "source": [
    "# Install maskrcnn-benchmark to extract detectron features\n",
    "import sys\n",
    "%cd ~/content\n",
    "!git clone https://gitlab.com/meetshah1995/vqa-maskrcnn-benchmark.git\n",
    "%cd ~/content/vqa-maskrcnn-benchmark\n",
    "# Compile custom layers and build mask-rcnn backbone\n",
    "!python setup.py build develop\n",
    "sys.path.append('~/content/vqa-maskrcnn-benchmark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('~/content/vqa-maskrcnn-benchmark/maskrcnn_benchmark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yEb_d9pcyYXY",
    "outputId": "60b13120-1f39-4393-e7a0-e9c41c291ef9"
   },
   "outputs": [],
   "source": [
    "#cat imports.py\n",
    "!pip install sentencepiece\n",
    "!pip install numpy\n",
    "!pip install git+https://github.com/PyTorchLightning/pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install intel-openmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yx6FeDEF2sw"
   },
   "source": [
    "## Demo\n",
    "\n",
    "The class handles everything from feature extraction, token extraction and predicting the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "UCD0nso8YelA",
    "outputId": "f3fecbb9-2300-442a-f444-b360565e6b7c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\53444\\content\\apex\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'maskrcnn_benchmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21224/1185303514.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmaskrcnn_benchmark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmaskrcnn_benchmark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmaskrcnn_benchmark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodeling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetector\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuild_detection_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'maskrcnn_benchmark'"
     ]
    }
   ],
   "source": [
    "%cd ~/content/apex\n",
    "import yaml\n",
    "import cv2\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from ipywidgets import widgets, Layout\n",
    "from io import BytesIO\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "from maskrcnn_benchmark.config import cfg\n",
    "from maskrcnn_benchmark.layers import nms\n",
    "from maskrcnn_benchmark.modeling.detector import build_detection_model\n",
    "from maskrcnn_benchmark.structures.image_list import to_image_list\n",
    "from maskrcnn_benchmark.utils.model_serialization import load_state_dict\n",
    "\n",
    "\n",
    "from mmf.datasets.processors.processors import VocabProcessor, VQAAnswerProcessor\n",
    "from mmf.models.pythia import Pythia\n",
    "from mmf.common.registry import registry\n",
    "from mmf.common.sample import Sample, SampleList\n",
    "from mmf.utils.env import setup_imports\n",
    "from mmf.utils.configuration import Configuration\n",
    "\n",
    "setup_imports()\n",
    "\n",
    "class MMFDemo:\n",
    "  TARGET_IMAGE_SIZE = [448, 448]\n",
    "  CHANNEL_MEAN = [0.485, 0.456, 0.406]\n",
    "  CHANNEL_STD = [0.229, 0.224, 0.225]\n",
    "  \n",
    "  def __init__(self):\n",
    "    self._init_processors()\n",
    "    self.pythia_model = self._build_pythia_model()\n",
    "    self.detection_model = self._build_detection_model()\n",
    "    self.resnet_model = self._build_resnet_model()\n",
    "    \n",
    "  def _init_processors(self):\n",
    "    args = Namespace()\n",
    "    args.opts = [\n",
    "        \"config=projects/pythia/configs/vqa2/defaults.yaml\",\n",
    "        \"datasets=vqa2\",\n",
    "        \"model=pythia\",\n",
    "        \"evaluation.predict=True\"\n",
    "    ]\n",
    "    args.config_override = None\n",
    "\n",
    "    configuration = Configuration(args=args)\n",
    "    \n",
    "    config = self.config = configuration.config\n",
    "    vqa_config = config.dataset_config.vqa2\n",
    "    text_processor_config = vqa_config.processors.text_processor\n",
    "    answer_processor_config = vqa_config.processors.answer_processor\n",
    "    \n",
    "    text_processor_config.params.vocab.vocab_file = \"~/content/model_data/vocabulary_100k.txt\"\n",
    "    answer_processor_config.params.vocab_file = \"~/content/model_data/answers_vqa.txt\"\n",
    "    # Add preprocessor as that will needed when we are getting questions from user\n",
    "    self.text_processor = VocabProcessor(text_processor_config.params)\n",
    "    self.answer_processor = VQAAnswerProcessor(answer_processor_config.params)\n",
    "\n",
    "    registry.register(\"vqa2_text_processor\", self.text_processor)\n",
    "    registry.register(\"vqa2_answer_processor\", self.answer_processor)\n",
    "    registry.register(\"vqa2_num_final_outputs\", \n",
    "                      self.answer_processor.get_vocab_size())\n",
    "    \n",
    "  def _build_pythia_model(self):\n",
    "    state_dict = torch.load('C:/Users/53444/content/model_data/pythia.pth')\n",
    "    model_config = self.config.model_config.pythia\n",
    "    model_config.model_data_dir = \"~/content/model_data/\"\n",
    "    model = Pythia(model_config)\n",
    "    model.build()\n",
    "    model.init_losses()\n",
    "    \n",
    "    if list(state_dict.keys())[0].startswith('module') and \\\n",
    "       not hasattr(model, 'module'):\n",
    "      state_dict = self._multi_gpu_state_to_single(state_dict)\n",
    "          \n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "  \n",
    "  def _build_resnet_model(self):\n",
    "    self.data_transforms = transforms.Compose([\n",
    "        transforms.Resize(self.TARGET_IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(self.CHANNEL_MEAN, self.CHANNEL_STD),\n",
    "    ])\n",
    "    resnet152 = models.resnet152(pretrained=True)\n",
    "    resnet152.eval()\n",
    "    modules = list(resnet152.children())[:-2]\n",
    "    self.resnet152_model = torch.nn.Sequential(*modules)\n",
    "    self.resnet152_model.to(\"cuda\")\n",
    "  \n",
    "  def _multi_gpu_state_to_single(self, state_dict):\n",
    "    new_sd = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if not k.startswith('module.'):\n",
    "            raise TypeError(\"Not a multiple GPU state of dict\")\n",
    "        k1 = k[7:]\n",
    "        new_sd[k1] = v\n",
    "    return new_sd\n",
    "  \n",
    "  def predict(self, url, question):\n",
    "    with torch.no_grad():\n",
    "      detectron_features = self.get_detectron_features(url)\n",
    "      resnet_features = self.get_resnet_features(url)\n",
    "\n",
    "      sample = Sample()\n",
    "\n",
    "      processed_text = self.text_processor({\"text\": question})\n",
    "      sample.text = processed_text[\"text\"]\n",
    "      sample.text_len = len(processed_text[\"tokens\"])\n",
    "\n",
    "      sample.image_feature_0 = detectron_features\n",
    "      sample.image_info_0 = Sample({\n",
    "          \"max_features\": torch.tensor(100, dtype=torch.long)\n",
    "      })\n",
    "\n",
    "      sample.image_feature_1 = resnet_features\n",
    "\n",
    "      sample_list = SampleList([sample])\n",
    "      sample_list = sample_list.to(\"cuda\")\n",
    "\n",
    "      scores = self.pythia_model(sample_list)[\"scores\"]\n",
    "      scores = torch.nn.functional.softmax(scores, dim=1)\n",
    "      actual, indices = scores.topk(5, dim=1)\n",
    "\n",
    "      top_indices = indices[0]\n",
    "      top_scores = actual[0]\n",
    "\n",
    "      probs = []\n",
    "      answers = []\n",
    "\n",
    "      for idx, score in enumerate(top_scores):\n",
    "        probs.append(score.item())\n",
    "        answers.append(\n",
    "            self.answer_processor.idx2word(top_indices[idx].item())\n",
    "        )\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return probs, answers\n",
    "    \n",
    "  \n",
    "  def _build_detection_model(self):\n",
    "\n",
    "      cfg.merge_from_file('~/content/model_data/detectron_model.yaml')\n",
    "      cfg.freeze()\n",
    "\n",
    "      model = build_detection_model(cfg)\n",
    "      checkpoint = torch.load('~/content/model_data/detectron_model.pth', \n",
    "                              map_location=torch.device(\"cpu\"))\n",
    "\n",
    "      load_state_dict(model, checkpoint.pop(\"model\"))\n",
    "\n",
    "      model.to(\"cuda\")\n",
    "      model.eval()\n",
    "      return model\n",
    "  \n",
    "  def get_actual_image(self, image_path):\n",
    "      if image_path.startswith('http'):\n",
    "          path = requests.get(image_path, stream=True).raw\n",
    "      else:\n",
    "          path = image_path\n",
    "      \n",
    "      return path\n",
    "\n",
    "  def _image_transform(self, image_path):\n",
    "      path = self.get_actual_image(image_path)\n",
    "\n",
    "      img = Image.open(path)\n",
    "      im = np.array(img).astype(np.float32)\n",
    "      im = im[:, :, ::-1]\n",
    "      im -= np.array([102.9801, 115.9465, 122.7717])\n",
    "      im_shape = im.shape\n",
    "      im_size_min = np.min(im_shape[0:2])\n",
    "      im_size_max = np.max(im_shape[0:2])\n",
    "      im_scale = float(800) / float(im_size_min)\n",
    "      # Prevent the biggest axis from being more than max_size\n",
    "      if np.round(im_scale * im_size_max) > 1333:\n",
    "           im_scale = float(1333) / float(im_size_max)\n",
    "      im = cv2.resize(\n",
    "           im,\n",
    "           None,\n",
    "           None,\n",
    "           fx=im_scale,\n",
    "           fy=im_scale,\n",
    "           interpolation=cv2.INTER_LINEAR\n",
    "       )\n",
    "      img = torch.from_numpy(im).permute(2, 0, 1)\n",
    "      return img, im_scale\n",
    "\n",
    "\n",
    "  def _process_feature_extraction(self, output,\n",
    "                                 im_scales,\n",
    "                                 feat_name='fc6',\n",
    "                                 conf_thresh=0.2):\n",
    "      batch_size = len(output[0][\"proposals\"])\n",
    "      n_boxes_per_image = [len(_) for _ in output[0][\"proposals\"]]\n",
    "      score_list = output[0][\"scores\"].split(n_boxes_per_image)\n",
    "      score_list = [torch.nn.functional.softmax(x, -1) for x in score_list]\n",
    "      feats = output[0][feat_name].split(n_boxes_per_image)\n",
    "      cur_device = score_list[0].device\n",
    "\n",
    "      feat_list = []\n",
    "\n",
    "      for i in range(batch_size):\n",
    "          dets = output[0][\"proposals\"][i].bbox / im_scales[i]\n",
    "          scores = score_list[i]\n",
    "\n",
    "          max_conf = torch.zeros((scores.shape[0])).to(cur_device)\n",
    "\n",
    "          for cls_ind in range(1, scores.shape[1]):\n",
    "              cls_scores = scores[:, cls_ind]\n",
    "              keep = nms(dets, cls_scores, 0.5)\n",
    "              max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep],\n",
    "                                           cls_scores[keep],\n",
    "                                           max_conf[keep])\n",
    "\n",
    "          keep_boxes = torch.argsort(max_conf, descending=True)[:100]\n",
    "          feat_list.append(feats[i][keep_boxes])\n",
    "      return feat_list\n",
    "\n",
    "  def masked_unk_softmax(self, x, dim, mask_idx):\n",
    "      x1 = F.softmax(x, dim=dim)\n",
    "      x1[:, mask_idx] = 0\n",
    "      x1_sum = torch.sum(x1, dim=1, keepdim=True)\n",
    "      y = x1 / x1_sum\n",
    "      return y\n",
    "   \n",
    "  def get_resnet_features(self, image_path):\n",
    "      path = self.get_actual_image(image_path)\n",
    "      img = Image.open(path).convert(\"RGB\")\n",
    "      img_transform = self.data_transforms(img)\n",
    "      \n",
    "      if img_transform.shape[0] == 1:\n",
    "        img_transform = img_transform.expand(3, -1, -1)\n",
    "      img_transform = img_transform.unsqueeze(0).to(\"cuda\")\n",
    "      \n",
    "      features = self.resnet152_model(img_transform).permute(0, 2, 3, 1)\n",
    "      features = features.view(196, 2048)\n",
    "      return features\n",
    "    \n",
    "  def get_detectron_features(self, image_path):\n",
    "      im, im_scale = self._image_transform(image_path)\n",
    "      img_tensor, im_scales = [im], [im_scale]\n",
    "      current_img_list = to_image_list(img_tensor, size_divisible=32)\n",
    "      current_img_list = current_img_list.to('cuda')\n",
    "      with torch.no_grad():\n",
    "          output = self.detection_model(current_img_list)\n",
    "      feat_list = self._process_feature_extraction(output, im_scales, \n",
    "                                                  'fc6', 0.2)\n",
    "      return feat_list[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3IanIVPt91G"
   },
   "source": [
    "### If the command below fails with 'CUDNN_EXECUTION_FAILED', try rerunning the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwF36OmQ72ir",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "demo = MMFDemo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CinwDLv5GLJI"
   },
   "source": [
    "## Use the text fields below to ask a question on an image\n",
    "\n",
    "Image URL can be any http/https URL. We show top 5 predictions from MMF. Confidence shows how confident MMF model was about a particular prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_55MoLFlhL4Y"
   },
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import pygame\n",
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import torchaudio\n",
    "from glob import glob\n",
    "from pydub import AudioSegment\n",
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 2\n",
    "RATE = 44100\n",
    "CHUNK = 1024\n",
    "RECORD_SECONDS = 10\n",
    "WAVE_OUTPUT_FILENAME = \"recording.wav\"\n",
    "\n",
    "def init_widgets(url, question):\n",
    "  image_text = widgets.Text(\n",
    "    description=\"Image URL\", layout=Layout(minwidth=\"70%\")\n",
    "  )\n",
    "  question_text = widgets.Text(\n",
    "      description=\"Question\", layout=Layout(minwidth=\"70%\")\n",
    "  )\n",
    "\n",
    "  image_text.value = url\n",
    "  question_text.value = question\n",
    "  submit_button = widgets.Button(description=\"Ask MMF!\")\n",
    "\n",
    "  display(image_text)\n",
    "  display(question_text)\n",
    "  display(submit_button)\n",
    "\n",
    "  submit_button.on_click(lambda b: on_button_click(\n",
    "      b, image_text, question_text\n",
    "  ))\n",
    "  \n",
    "  return image_text, question_text\n",
    "  \n",
    "def on_button_click(b, image_text, question_text):\n",
    "  #recording part\n",
    "  audio = pyaudio.PyAudio()\n",
    "  stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                    rate=RATE, input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "  print(\"recording...\")\n",
    "  frames = []\n",
    "  for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "  print(\"finished recording\")\n",
    "  stream.stop_stream()\n",
    "  stream.close()\n",
    "  audio.terminate()\n",
    "  waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "  waveFile.setnchannels(CHANNELS)\n",
    "  waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "  waveFile.setframerate(RATE)\n",
    "  waveFile.writeframes(b''.join(frames))\n",
    "  waveFile.close()\n",
    "\n",
    "  #stt part\n",
    "  s = \"\"\n",
    "\n",
    "  #wav_audio = AudioSegment.from_file(\"Recording.m4a\", format=\"m4a\")\n",
    "  #wav_audio.export(\"recording.wav\", format=\"wav\")    \n",
    "\n",
    "  device = torch.device('cpu')  # gpu also works, but our models are fast enough for CPU\n",
    "\n",
    "  model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models',\n",
    "                                        model='silero_stt',\n",
    "                                        language='en', # also available 'de', 'es'\n",
    "                                        device=device)\n",
    "  (read_batch, split_into_batches,\n",
    "  read_audio, prepare_model_input) = utils  # see function signature for details\n",
    "\n",
    "  test_files = glob('recording.wav')\n",
    "  batches = split_into_batches(test_files, batch_size=10)\n",
    "  input = prepare_model_input(read_batch(batches[0]),\n",
    "                              device=device)\n",
    "\n",
    "  output = model(input)\n",
    "  for example in output:\n",
    "      s = decoder(example.cpu())\n",
    "\n",
    "  #output part\n",
    "  clear_output()\n",
    "  image_path = demo.get_actual_image(image_text.value)\n",
    "  image = Image.open(image_path)\n",
    "  \n",
    "  #scores, predictions = demo.predict(image_text.value, question_text.value)\n",
    "  scores, predictions = demo.predict(image_text.value, s)\n",
    "  scores = [score * 100 for score in scores]\n",
    "  df = pd.DataFrame({\n",
    "      \"Prediction\": predictions,\n",
    "      \"Confidence\": scores\n",
    "  })\n",
    "  language = 'en'\n",
    "  myobj = gTTS(text=predictions[0], lang=language, slow=False)\n",
    "  myobj.save(\"output.mp3\")\n",
    "  pygame.mixer.init()\n",
    "  pygame.mixer.music.load('output.mp3')\n",
    "  pygame.mixer.music.play()\n",
    "  init_widgets(image_text.value, question_text.value)\n",
    "  display(image)\n",
    "  display(HTML(df.to_html()))\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "image_text, question_text = init_widgets(\n",
    "    \"VizWiz_val_00028000.jpg\", \n",
    "    s\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "kXeEePmeEuuY",
    "l4S01cUqE3WJ",
    "ivSdn9BFFpxp",
    "B5o-zqvuFxeR",
    "8yx6FeDEF2sw"
   ],
   "name": "mmf_demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
